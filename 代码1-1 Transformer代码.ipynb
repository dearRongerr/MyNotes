{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "\n",
    "# step1 --- 定义常量 ---\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小，单词表中有多少个单词\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "\n",
    "# 词嵌入维度\n",
    "model_dim = 8\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "# batch size = 2,源 第一个句子长度 = 2，第二个句子长度 = 4 \n",
    "src_len = torch.Tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4,3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成源句子和目标句子，构建batch，并且做padding，默认值为0\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),\n",
    "                                           (0,max(src_len)-L)),0)\n",
    "                                           for L in src_len])\n",
    "\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_tgt_words,(L,)),\n",
    "                                           (0,max(tgt_len)-L)),0)\n",
    "                                           for L in tgt_len])\n",
    "\n",
    "# step2  构造word embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1,model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1,model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# step3 构造position embedding\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1,1))\n",
    "i_mat = torch.pow(10000,torch.arange(0,8,2).reshape((1,-1))/model_dim)\n",
    "\n",
    "pe_embedding_table = torch.zeros(max_position_len,model_dim)\n",
    "pe_embedding_table[:,0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:,1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "# print(pe_embedding_table)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len,model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table,requires_grad=False)\n",
    "\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)),0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "# print(src_pos)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "# print(src_pe_embedding)\n",
    "# print(tgt_pe_embedding)\n",
    "\n",
    "# step4  构造 encoder的self attention mask\n",
    "# mask的shape：[batch_size,max_src_len,max_src_len]，值为1或-inf\n",
    "\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(src_len)-L)),0) \n",
    "                                               for L in src_len]),2)\n",
    "\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(1,2))\n",
    "\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix\n",
    "\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "score = torch.randn(batch_size,max(src_len),max(src_len))\n",
    "# print(score.shape,mask_encoder_self_attention.shape)\n",
    "\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention,-1e9)\n",
    "prob = F.softmax(masked_score,-1)\n",
    "\n",
    "# step5：构造intra attention 的mask\n",
    "# Q @ K^T shape:[batch_size ,tgt_seq_len,src_seq_len]\n",
    "\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(src_len)-L)),0)\n",
    "                                                for L in src_len]),2)\n",
    "\n",
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(tgt_len)-L)),0)\n",
    "                                                for L in tgt_len]),2)\n",
    "\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(1,2))\n",
    "invalid_cross_pos_matrix = 1- valid_cross_pos_matrix\n",
    "mask_cross_self_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "# print(mask_cross_self_attention)\n",
    "\n",
    "# step6 构造decoder self-attention 的mask\n",
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),\n",
    "                                                          (0,max(tgt_len)-L,0,max(tgt_len)-L)),0)\n",
    "                                                          for L in tgt_len])\n",
    "invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "# print(invalid_decoder_tri_matrix)\n",
    "\n",
    "score = torch.randn(batch_size,max(tgt_len),max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix,-1e09)\n",
    "prob = F.softmax(masked_score,-1)\n",
    "# print(tgt_len)\n",
    "# print(prob)\n",
    "\n",
    "\n",
    "# step7 构建scaled self-attention\n",
    "def scaled_dot_product_attention(Q,K,V,attn_mask):\n",
    "    # shape of Q,K,V:(batch_size*num_head,seq_len,model_dim/num_head)\n",
    "    score = torch.bmm(Q,K.transpose(-2,-1))/torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attn_mask,-1e9)\n",
    "    prob  = F.softmax(masked_score,-1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
