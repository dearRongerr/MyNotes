{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现LSTM & LSTMP源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "bs,T,i_size,h_size = 2,3,4,5\n",
    "# proj_size\n",
    "input = torch.randn(bs,T,i_size) # 输入序列\n",
    "c0 = torch.randn(bs,h_size)  # 初始值不需要训练\n",
    "h0 = torch.randn(bs,h_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "# 调用官方LSTM API\n",
    "lstm_layer = nn.LSTM(i_size,h_size,batch_first=True)\n",
    "output,(h_finall,c_finall) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0)))\n",
    "\n",
    "for k,v in lstm_layer.named_parameters():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写一个LSTM\n",
    "def lstm_forward(input,initial_states,w_ih,w_hh,b_ih,b_hh):\n",
    "    # 以上写好了 函数签名\n",
    "    h0,c0 = initial_states #初始状态\n",
    "    bs,T,i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1)\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs,T,output_size) # 输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:]  # 当前时刻的输入向量，[bs,i_size]\n",
    "\n",
    "        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-1))  #[bs,4*h_size,1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs,4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-1))  #[bs,4*h_size,1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  # [bs,4*h_size]\n",
    "\n",
    "        # 分别计算 输入门(i)，遗忘门(f)，cell门(g)，输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:,h_size:2*h_size] + w_times_h_prev[:,h_size:2*h_size]+\n",
    "                            b_ih[h_size:2*h_size]+b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:,2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]+\n",
    "                            b_ih[2*h_size:3*h_size]+b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:,3*h_size:4*h_size] + w_times_h_prev[:,3*h_size:4*h_size]+\n",
    "                            b_ih[3*h_size:4*h_size]+b_hh[3*h_size:4*h_size])\n",
    "\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output,(prev_h,prev_c)\n",
    "\n",
    "output_custom,(h_finall_custom,c_finall_custom) = lstm_forward(input,(h0,c0),lstm_layer.weight_ih_l0,\n",
    "                                                               lstm_layer.weight_hh_l0,\n",
    "                                                               lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(output,output_custom))\n",
    "print(torch.allclose(h_finall,h_finall_custom))\n",
    "print(torch.allclose(c_finall,c_finall_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义常量\n",
    "bs,T,i_size,h_size = 2,3,4,5\n",
    "proj_size = 3\n",
    "input = torch.randn(bs,T,i_size) # 输入序列\n",
    "c0 = torch.randn(bs,h_size)  # 初始值不需要训练\n",
    "h0 = torch.randn(bs,proj_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([1, 2, 3]) torch.Size([1, 2, 5])\n",
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 调用官方LSTM API\n",
    "lstm_layer = nn.LSTM(i_size,h_size,batch_first=True,proj_size = proj_size)\n",
    "output,(h_finall,c_finall) = lstm_layer(input,(h0.unsqueeze(0),c0.unsqueeze(0)))\n",
    "\n",
    "print(output.shape,h_finall.shape,c_finall.shape)\n",
    "\n",
    "for k,v in lstm_layer.named_parameters():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己写一个LSTM\n",
    "def lstm_forward(input,initial_states,w_ih,w_hh,b_ih,b_hh,w_hr=None):\n",
    "    # 以上写好了 函数签名\n",
    "    h0,c0 = initial_states #初始状态\n",
    "    bs,T,i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs,1,1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs,1,1)\n",
    "\n",
    "    if w_hr is not None:\n",
    "        p_size = w_hr.shape[0]\n",
    "        output_size = p_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(bs,1,1)  # [bs,p_size,h_size]\n",
    "    else:\n",
    "        output_size = h_size\n",
    "\n",
    "    output = torch.zeros(bs,T,output_size) # 输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:]  # 当前时刻的输入向量，[bs,i_size]\n",
    "\n",
    "        w_times_x = torch.bmm(batch_w_ih,x.unsqueeze(-1))  #[bs,4*h_size,1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs,4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh,prev_h.unsqueeze(-1))  #[bs,4*h_size,1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  # [bs,4*h_size]\n",
    "\n",
    "        # 分别计算 输入门(i)，遗忘门(f)，cell门(g)，输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:,:h_size] + w_times_h_prev[:,:h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:,h_size:2*h_size] + w_times_h_prev[:,h_size:2*h_size]+\n",
    "                            b_ih[h_size:2*h_size]+b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:,2*h_size:3*h_size] + w_times_h_prev[:,2*h_size:3*h_size]+\n",
    "                            b_ih[2*h_size:3*h_size]+b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:,3*h_size:4*h_size] + w_times_h_prev[:,3*h_size:4*h_size]+\n",
    "                            b_ih[3*h_size:4*h_size]+b_hh[3*h_size:4*h_size])\n",
    "\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        if w_hr is not None: # 做projection\n",
    "            prev_h = torch.bmm(batch_w_hr,prev_h.unsqueeze(-1)) # [bs,p_size,1]\n",
    "            prev_h = prev_h.squeeze(-1) # bs× p_size\n",
    "             \n",
    "\n",
    "        output[:,t,:] = prev_h\n",
    "\n",
    "    return output,(prev_h,prev_c)\n",
    "\n",
    "output_custom,(h_finall_custom,c_finall_custom) = lstm_forward(input,(h0,c0),lstm_layer.weight_ih_l0,\n",
    "                                                               lstm_layer.weight_hh_l0,\n",
    "                                                               lstm_layer.bias_ih_l0,lstm_layer.bias_hh_l0,\n",
    "                                                               lstm_layer.weight_hr_l0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(output,output_custom))\n",
    "print(torch.allclose(h_finall,h_finall_custom))\n",
    "print(torch.allclose(c_finall,c_finall_custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
