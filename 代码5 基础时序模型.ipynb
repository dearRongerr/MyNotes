{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,T=2,3  # 批大小，输入序列长度\n",
    "input_size,hidden_size = 2,3 # 输入特征大小，隐含层特征大小\n",
    "input = torch.randn(bs,T,input_size)  # 随机初始化一个输入特征序列\n",
    "h_prev = torch.zeros(bs,hidden_size) # 初始隐含状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7709,  0.7301, -0.9299],\n",
      "         [-0.6976, -0.8241, -0.1903],\n",
      "         [-0.6485, -0.2633, -0.1093]],\n",
      "\n",
      "        [[-0.2035,  0.7439, -0.1369],\n",
      "         [-0.4805, -0.5790,  0.1787],\n",
      "         [-0.6185,  0.4854, -0.4907]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.6485, -0.2633, -0.1093],\n",
      "         [-0.6185,  0.4854, -0.4907]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# step1 调用pytorch RNN API\n",
    "rnn = nn.RNN(input_size,hidden_size,batch_first=True)\n",
    "rnn_output,state_finall = rnn(input,h_prev.unsqueeze(0))\n",
    "\n",
    "print(rnn_output)\n",
    "print(state_finall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 手写 rnn_forward函数，实现RNN的计算原理\n",
    "def rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev):\n",
    "    bs,T,input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs,T,h_dim) # 初始化一个输出（状态）矩阵\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:,t,:].unsqueeze(2)  # 获取当前时刻的输入特征，bs*input_size*1\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs,1,1) # bs * h_dim * input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs,1,1)# bs * h_dim * h_dim\n",
    "\n",
    "        w_times_x = torch.bmm(w_ih_batch,x).squeeze(-1) # bs*h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch,h_prev.unsqueeze(2)).squeeze(-1) # bs*h_him\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "\n",
    "        h_out[:,t,:] = h_prev\n",
    "\n",
    "    return h_out,h_prev.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7709,  0.7301, -0.9299],\n",
      "         [-0.6976, -0.8241, -0.1903],\n",
      "         [-0.6485, -0.2633, -0.1093]],\n",
      "\n",
      "        [[-0.2035,  0.7439, -0.1369],\n",
      "         [-0.4805, -0.5790,  0.1787],\n",
      "         [-0.6185,  0.4854, -0.4907]]], grad_fn=<CopySlices>)\n",
      "tensor([[[-0.6485, -0.2633, -0.1093],\n",
      "         [-0.6185,  0.4854, -0.4907]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 验证结果\n",
    "custom_rnn_output,custom_state_finall = rnn_forward(input,\n",
    "                                                    rnn.weight_ih_l0,\n",
    "                                                    rnn.weight_hh_l0,\n",
    "                                                    rnn.bias_ih_l0,\n",
    "                                                    rnn.bias_hh_l0,\n",
    "                                                    h_prev)\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_finall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(rnn_output,custom_rnn_output))\n",
    "print(torch.allclose(state_finall,custom_state_finall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.5458,  0.5512],\n",
      "        [-0.5077, -0.0750],\n",
      "        [ 0.3572,  0.1419]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.4093,  0.2012,  0.0746],\n",
      "        [-0.5619, -0.3820, -0.4060],\n",
      "        [-0.4412,  0.2706, -0.2816]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.5063, -0.1391, -0.0587], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.0343, -0.2352,  0.3234], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[ 0.1298,  0.5538],\n",
      "        [ 0.4151,  0.2533],\n",
      "        [-0.4401,  0.5322]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[-0.4232,  0.2246,  0.4265],\n",
      "        [ 0.3016, -0.4142, -0.3064],\n",
      "        [-0.1960,  0.2845,  0.3770]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([-0.4372, -0.2452,  0.4506], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([ 0.3957, -0.4655, -0.2143], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# step3 手写一个 bidirectional_rnn_forward函数，实现双向RNN的计算原理\n",
    "def bidirectional_rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev,\n",
    "                              weight_ih_reverse,weight_hh_reverse,bias_ih_reverse,\n",
    "                              bias_hh_reverse,h_prev_reverse):\n",
    "    bs,T,input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs,T,h_dim*2) # 初始化一个输出（状态）矩阵，注意双向是两倍的特征大小\n",
    "\n",
    "    forward_output = rnn_forward(input,weight_ih,weight_hh,bias_ih,bias_hh,h_prev)[0]  # forward layer\n",
    "    backward_output = rnn_forward(torch.flip(input,[1]),weight_ih_reverse,weight_hh_reverse,bias_ih_reverse, bias_hh_reverse,h_prev_reverse)[0] # backward layer\n",
    "\n",
    "    # 将input按照时间的顺序翻转\n",
    "    h_out[:,:,:h_dim] = forward_output\n",
    "    h_out[:,:,h_dim:] = torch.flip(backward_output,[1]) #需要再翻转一下 才能和forward output拼接\n",
    "\n",
    "    \n",
    "    h_n = torch.zeros(bs,2,h_dim)  # 要最后的状态连接\n",
    "\n",
    "    h_n[:,0,:] = forward_output[:,-1,:]\n",
    "    h_n[:,1,:] = backward_output[:,-1,:]\n",
    "\n",
    "    h_n = h_n.transpose(0,1)\n",
    "\n",
    "    return h_out,h_n\n",
    "    # return h_out,h_out[:,-1,:].reshape((bs,2,h_dim)).transpose(0,1)\n",
    "\n",
    "# 验证一下 bidirectional_rnn_forward的正确性\n",
    "bi_rnn = nn.RNN(input_size,hidden_size,batch_first=True,bidirectional=True)\n",
    "h_prev = torch.zeros((2,bs,hidden_size))\n",
    "bi_rnn_output,bi_state_finall = bi_rnn(input,h_prev)\n",
    "\n",
    "for k,v in bi_rnn.named_parameters():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bi_rnn_output,custom_bi_state_finall = bidirectional_rnn_forward(input,\n",
    "                                                                        bi_rnn.weight_ih_l0,\n",
    "                                                                        bi_rnn.weight_hh_l0,\n",
    "                                                                        bi_rnn.bias_ih_l0,\n",
    "                                                                        bi_rnn.bias_hh_l0,\n",
    "                                                                        h_prev[0],\n",
    "                                                                        bi_rnn.weight_ih_l0_reverse,\n",
    "                                                                        bi_rnn.weight_hh_l0_reverse,\n",
    "                                                                        bi_rnn.bias_ih_l0_reverse,\n",
    "                                                                        bi_rnn.bias_hh_l0_reverse,\n",
    "                                                                        h_prev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch API output\n",
      "tensor([[[-0.8470,  0.5436, -0.3571,  0.0393, -0.8730,  0.9124],\n",
      "         [ 0.5724, -0.0194,  0.7805,  0.5884, -0.2443,  0.7351],\n",
      "         [-0.4845, -0.7670, -0.1836,  0.0907, -0.5768,  0.3587]],\n",
      "\n",
      "        [[-0.9186, -0.4089,  0.0847, -0.9221, -0.7344, -0.9120],\n",
      "         [ 0.3084, -0.3562,  0.7382, -0.0584, -0.1021, -0.5778],\n",
      "         [-0.8156, -0.4316, -0.3803, -0.3811, -0.7703,  0.1212]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.4845, -0.7670, -0.1836],\n",
      "         [-0.8156, -0.4316, -0.3803]],\n",
      "\n",
      "        [[ 0.0393, -0.8730,  0.9124],\n",
      "         [-0.9221, -0.7344, -0.9120]]], grad_fn=<StackBackward0>)\n",
      "\n",
      " custom bidirectional_rnn_forward function output:\n",
      "tensor([[[-0.8470,  0.5436, -0.3571,  0.0393, -0.8730,  0.9124],\n",
      "         [ 0.5724, -0.0194,  0.7805,  0.5884, -0.2443,  0.7351],\n",
      "         [-0.4845, -0.7670, -0.1836,  0.0907, -0.5768,  0.3587]],\n",
      "\n",
      "        [[-0.9186, -0.4089,  0.0847, -0.9221, -0.7344, -0.9120],\n",
      "         [ 0.3084, -0.3562,  0.7382, -0.0584, -0.1021, -0.5778],\n",
      "         [-0.8156, -0.4316, -0.3803, -0.3811, -0.7703,  0.1212]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[-0.4845, -0.7670, -0.1836],\n",
      "         [-0.8156, -0.4316, -0.3803]],\n",
      "\n",
      "        [[ 0.0393, -0.8730,  0.9124],\n",
      "         [-0.9221, -0.7344, -0.9120]]], grad_fn=<TransposeBackward0>)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch API output\")\n",
    "print(bi_rnn_output)\n",
    "print(bi_state_finall)\n",
    "\n",
    "print(\"\\n custom bidirectional_rnn_forward function output:\")\n",
    "print(custom_bi_rnn_output)\n",
    "print(custom_bi_state_finall)\n",
    "print(torch.allclose(bi_rnn_output,custom_bi_rnn_output))\n",
    "print(torch.allclose(bi_state_finall,custom_bi_state_finall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
